{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cc7aec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wojte\\anaconda3\\envs\\pytorch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer, TrainingArguments, Trainer, AutoModelForSequenceClassification\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ec7de58",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('./IMDB_dataset/IMDB dataset.csv')\n",
    "\n",
    "df.sentiment.replace(\"positive\" , 1 , inplace = True)\n",
    "df.sentiment.replace(\"negative\" , 0 , inplace = True)\n",
    "df = df.rename(columns={\"review\": \"text\", \"sentiment\":\"label\"}) \n",
    "\n",
    "train, test= train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fc81a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = Dataset.from_pandas(train, preserve_index=False)\n",
    "dataset_test = Dataset.from_pandas(test, preserve_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06cf51f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 40000\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9314f32e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"I did not watch the entire movie. I could not watch the entire movie. I stopped the DVD after watching for half an hour and I suggest anyone thinking of watching themselves it stop themselves before taking the disc out of the case.<br /><br />I like Mafia movies both tragic and comic but Corky Romano can only be described as a tragic attempt at a mafia comedy.<br /><br />The problem is Corky Romano simply tries too hard to get the audience to laugh, the plot seems to be an excuse for moving Chris Kattan (Corky) from one scene to another. Corky himself is completely overplayed and lacks subtlety or credulity - all his strange mannerisms come across as contrived - Chris Kattan is clearly 'acting' rather than taking a role - it bounces you right out of the story. Each scene is utterly predictable, the 'comedic event' that will occur on the set is obvious as soon as each scene is introduced. In comedies such as Mr. Bean the disasters caused by the title character are funny because you can empathise with the characters motivations and initial event and the situation the character ends up in is not telegraphed. Corky however gives the feeling that he is deliberately screwing up in a desperate attempt to draw a laugh from the audience.<br /><br />If Chris had not played such an alien character (who never really connects with the other characters in the movie) and whose behaviour is entirely inexplicable (except for trying to draw laughs) and the comedy scenes weren't so predictable and stereotyped - all the jokes seemed far too familiar) this movie could have been watchable. But it isn't. Don't watch it.\",\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7423419e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6cfb4e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sample):\n",
    "    return tokenizer(sample[\"text\"], padding=\"max_length\", truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d7c66c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    }
   ],
   "source": [
    "dataset_train_tokenized = dataset_train.map(tokenize,batched=True)\n",
    "dataset_test_tokenized = dataset_test.map(tokenize,batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3aeef1c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 40000\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12b42a44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_test_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1554a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_dataset = dataset_train_tokenized.shuffle(seed=42).select(range(25000))\n",
    "#small_eval_dataset = dataset_test_tokenized.shuffle(seed=42).select(range(10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3eb7f076",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBertForSequenceClassification(\n",
      "  (distilbert): DistilBertModel(\n",
      "    (embeddings): Embeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (layer): ModuleList(\n",
      "        (0-5): 6 x TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f880a9",
   "metadata": {},
   "source": [
    "Trainer does not automatically evaluate model performance during training. Youâ€™ll need to pass Trainer a function to compute and report metrics.\n",
    "\n",
    "Accuracy is the proportion of correct predictions among the total number of cases processed. It can be computed with: Accuracy = (TP + TN) / (TP + TN + FP + FN) Where: TP: True positive TN: True negative FP: False positive FN: False negative\n",
    "\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "results = accuracy_metric.compute(references=[0, 1], predictions=[0, 1])\n",
    "\n",
    "print(results)\n",
    "\n",
    "{'accuracy': 1.0}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7acad1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d11e1b",
   "metadata": {},
   "source": [
    "axis = -1\n",
    "\n",
    "This means that the index that will be returned by argmax will be taken from the last axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7473d12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0fa1e0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(output_dir=\"bert_trainer\", \n",
    "                                  evaluation_strategy=\"epoch\",\n",
    "                                  save_strategy=\"epoch\",\n",
    "                                  per_device_train_batch_size=4,\n",
    "                                  per_device_eval_batch_size=4,\n",
    "                                  num_train_epochs = 2,\n",
    "                                  learning_rate = 2e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "86ff634c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=dataset_test_tokenized,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2648714e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "40efac7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wojte\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12500' max='12500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12500/12500 2:02:40, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.324900</td>\n",
       "      <td>0.367570</td>\n",
       "      <td>0.913200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.180700</td>\n",
       "      <td>0.348561</td>\n",
       "      <td>0.929800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=12500, training_loss=0.2793598193359375, metrics={'train_runtime': 7366.9472, 'train_samples_per_second': 6.787, 'train_steps_per_second': 1.697, 'total_flos': 6623369932800000.0, 'train_loss': 0.2793598193359375, 'epoch': 2.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8318bce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./bert_trainer/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51791b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained('./bert_trainer/tokenizer/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "922fdf74",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loaded = AutoModelForSequenceClassification.from_pretrained(\"./bert_trainer/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "615c194e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer2 = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42dc42b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0378b86c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n"
     ]
    }
   ],
   "source": [
    "sentiment_model = pipeline(\"text-classification\", model=model_loaded, tokenizer=tokenizer2, top_k=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e88f6d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"Very BAD movie!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0eabee11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'label': 'LABEL_0', 'score': 0.999428927898407},\n",
       "  {'label': 'LABEL_1', 'score': 0.0005710835102945566}]]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_model(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "07c51c01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'label': 'LABEL_0', 'score': 0.00046284840209409595},\n",
       "  {'label': 'LABEL_1', 'score': 0.9995372295379639}]]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1 = \"Very Good movie!\"\n",
    "sentiment_model(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "64e71bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"BAD film!\"\n",
    "res = sentiment_model(text1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5251c322",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_0', 'score': 0.9995126724243164},\n",
       " {'label': 'LABEL_1', 'score': 0.00048729247646406293}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c2c3e704",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentiment in res:\n",
    "    if sentiment['label'] == 'LABEL_1':\n",
    "        pos = sentiment['score']\n",
    "    elif sentiment['label'] == 'LABEL_0':\n",
    "        neg = sentiment['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "99f00230",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9995126724243164"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "acecb5a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00048729247646406293"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
